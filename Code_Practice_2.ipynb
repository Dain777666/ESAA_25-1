{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuM/F79fHUtihMrdkvvnmC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dain777666/ESAA_25-1/blob/main/Code_Practice_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfjpC6tpUtCh"
      },
      "outputs": [],
      "source": [
        "# 라이브러리 임포트\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST, CIFAR10, CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## data loader\n",
        "\n",
        "path = './datasets/'\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_data = MNIST(root=path,train=True,transform=transform,download=True)\n",
        "test_data = MNIST(root=path,train=False,transform=transform,download=True)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True,num_workers=4)\n",
        "test_loader = DataLoader(dataset=test_data,batch_size=batch_size,shuffle=False,num_workers=4)\n",
        "\n",
        "input_shape = train_data[0][0].shape # 1, 28, 28\n",
        "output_shape = len(train_data.classes) # 10"
      ],
      "metadata": {
        "id": "eA7s2hA2WVvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## model definition\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2) # stride=1(default)\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=(2,2), stride=2, padding=0) # padding=0(default)\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=(2,2), stride=2, padding=0)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(400,120)\n",
        "        self.fc2 = nn.Linear(120,84)\n",
        "        self.fc3 = nn.Linear(84,output_shape)\n",
        "        # conv는 재사용 불가(학습이 수행되기 때문에)/pool, dropout은 재사용 가능\n",
        "\n",
        "    def forward(self,x):\n",
        "      # -------Feature Extraction-------\n",
        "      # print(x.shape): 100, 1, 28, 28\n",
        "        hidden = F.leaky_relu(self.conv1(x))\n",
        "      # print(hidden.shape): 100, 6, 28, 28\n",
        "        hidden = self.pool1(hidden)\n",
        "      # print(hidden.shape): 100, 6, 14, 14\n",
        "        hidden = F.leaky_relu(self.conv2(hidden))\n",
        "      # print(hidden.shape): 100, 16, 10, 10\n",
        "        hidden = self.pool2(hidden)\n",
        "      # print(hidden.shape): 100, 16, 5, 5\n",
        "        hidden = self.flatten(hidden)\n",
        "\n",
        "      # -------Classification---------\n",
        "      # print(hidden.shape): 100, 400\n",
        "        hidden = F.leaky_relu(self.fc1(hidden))\n",
        "      # print(hidden.shape): 100, 120\n",
        "        hidden = F.leaky_relu(self.fc2(hidden))\n",
        "      # print(hidden.shpae): 100, 84\n",
        "        output = self.fc3(hidden)\n",
        "      # print(output.shape): 100, 10\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "2FsInNVA2jfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps:0\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model = LeNet().to(device) # 이 부분을 제외하고는 이전 코드와 동일\n",
        "loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
        "\n",
        "num_epoch = 100\n",
        "train_loss_list, test_loss_list = list(), list()\n",
        "\n",
        "for i in range(num_epoch):\n",
        "\n",
        "    # train\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(train_loader):\n",
        "\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_est = model.forward(x)\n",
        "        cost = loss(y_est, y)\n",
        "\n",
        "        total_loss == cost.item()*len(x)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(y_est, dim=1)\n",
        "        count = (pred == y).sum().item()\n",
        "\n",
        "    acc = count/len(train_data)\n",
        "    ave_loss = total_loss/len(train_data)\n",
        "\n",
        "    train_loss_list.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(\"\\nEpoch %d Train: %.3f / %.3f\"%(i,ave_loss,acc))\n",
        "\n",
        "    # eval\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x, y) in enumerate(test_loader):\n",
        "\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_est = model.forward(x)\n",
        "            cost = loss(y_est, y)\n",
        "\n",
        "            total_loss == cost.item()*len(x)\n",
        "\n",
        "            pred = torch.argmax(y_est, dim=1)\n",
        "            count = (pred == y).sum().item()\n",
        "\n",
        "    acc = count/len(train_data)\n",
        "    ave_loss = total_loss/len(train_data)\n",
        "\n",
        "    train_loss_list.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(\"Epoch %d Test: %.3f / %.3f\"%(i,ave_loss,acc))\n",
        "\n",
        "\n",
        "print()\n",
        "num_parameter = 0\n",
        "for parameter in model.parameters():\n",
        "    print(parameter.shape)\n",
        "    num_parameter += np.prod(parameter.size())\n",
        "print(num_parameter)\n",
        "\n",
        "# SoftmaxClassifier와 비교해보면 파라미터 개수가 엄청 차이남?"
      ],
      "metadata": {
        "id": "LVJmVy9I_AUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR-100 Dataset\n",
        "\n",
        "## data loader\n",
        "\n",
        "path = './datasets/'\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), # 0~1로 Rescaling\n",
        "                                transform.Normalize(mean=[0.5071, 0.4867, 0.4408],\n",
        "                                                    std=[0.2675, 0.2565, 0.2761])])\n",
        "\n",
        "train_data = CIFAR100(root=path,train=True,transform=transform,download=True)\n",
        "test_data = CIFAR100(root=path,train=False,transform=transform,download=True)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True,num_workers=4)\n",
        "test_loader = DataLoader(dataset=test_data,batch_size=batch_size,shuffle=False,num_workers=4)\n",
        "\n",
        "input_shape = train_data[0][0].shape # 3 * 32 * 32\n",
        "output_shape = len(train_data.classes) # 100"
      ],
      "metadata": {
        "id": "neq28t9oCd_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## model definition\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #\n",
        "        self.fe = nn.Sequential( # 100, 3, 32, 32 -> if 3*32*32=3000 -> 500 -> 500\n",
        "                    nn.Conv2d(in_channels=3, out_channels=9, kernel_size=(3,3), stride=1, padding=1), # 100, 9, 32, 32\n",
        "                    nn.LeakyReLU(),\n",
        "                    nn.Conv2d(in_channels=9, out_channels=18, kernel_size=(5,5), stride=1, padding=2), # 100, 18, 32, 32\n",
        "                    nn.LeakyReLU(),\n",
        "                    nn.AvgPool2d(kernel_size=(2,2), stride=2, padding=0), # 100, 18, 16, 16\n",
        "                    nn.LeakyReLU(),\n",
        "                    nn.AvgPool2d(kernel_size=(2,2), stride=2, padding=0)) # 100, 32, 4, 4\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.fcs = nn.Sequential(\n",
        "                    nn.Linear(512,256),\n",
        "                    nn.LeakyReLU(),\n",
        "                    nn.Linear(120,84),\n",
        "                    nn.Dropout(),\n",
        "                    nn.Linear(256,output_shape))\n",
        "\n",
        "    def forward(self,x):\n",
        "      hidden = self.fe(x)\n",
        "      hidden = self.flatten(hidden)\n",
        "      output = self.fcs(hidden)\n",
        "\n",
        "      return output"
      ],
      "metadata": {
        "id": "Ow9xZ3yKDDFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps:0\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model = LeNet().to(device) # 이 부분을 제외하고는 이전 코드와 동일\n",
        "loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
        "\n",
        "num_epoch = 100\n",
        "train_loss_list, test_loss_list = list(), list()\n",
        "\n",
        "for i in range(num_epoch):\n",
        "\n",
        "    # train\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(train_loader):\n",
        "\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_est = model.forward(x)\n",
        "        cost = loss(y_est, y)\n",
        "\n",
        "        total_loss == cost.item()*len(x)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(y_est, dim=1)\n",
        "        count = (pred == y).sum().item()\n",
        "\n",
        "    acc = count/len(train_data)\n",
        "    ave_loss = total_loss/len(train_data)\n",
        "\n",
        "    train_loss_list.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(\"\\nEpoch %d Train: %.3f / %.3f\"%(i,ave_loss,acc))\n",
        "\n",
        "    # eval\n",
        "    model.eval() # dropout이 들어갔으니까??\n",
        "\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x, y) in enumerate(test_loader):\n",
        "\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_est = model.forward(x)\n",
        "            cost = loss(y_est, y)\n",
        "\n",
        "            total_loss == cost.item()*len(x)\n",
        "\n",
        "            pred = torch.argmax(y_est, dim=1)\n",
        "            count = (pred == y).sum().item()\n",
        "\n",
        "    acc = count/len(train_data)\n",
        "    ave_loss = total_loss/len(train_data)\n",
        "\n",
        "    train_loss_list.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(\"Epoch %d Test: %.3f / %.3f\"%(i,ave_loss,acc))\n",
        "\n",
        "\n",
        "print()\n",
        "num_parameter = 0\n",
        "for parameter in model.parameters():\n",
        "    print(parameter.shape)\n",
        "    num_parameter += np.prod(parameter.size())\n",
        "print(num_parameter)"
      ],
      "metadata": {
        "id": "3uAJ3u5-Fz-Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}